---
published: true
title: Kedro for ML industrialization
collection: cv
layout: single
author_profile: true
read_time: true
categories: [projects]
header :
    teaser : /assets/images/kedro_banner.png
comments : true
toc: true
toc_sticky: true
sidebar:
    nav: sidebar-sample
---

In the following article, i will show how to install and use [Kedro](https://github.com/quantumblacklabs/kedro), an open source Python library that will simplify and clarify the way to define data pipelines for your projects. 

Then we will create a project and a custom kedro dataset 
 

# Install kedro

First create a new  conda virtual environment :
kedro is compatible with python 3.6, 3.7 and 3.8 versions. Here i choosed 3.7.
```bash
conda create -n kedro_project python=3.7
```

Then activate it
```bash
conda activate kedro_project
```

Install kedro from conda-forge repository:


```bash
conda install -c conda-forge kedro
```

Install also kedro-viz plugin, very usefull to display your data pipelines:

## Install kedro-viz plugin

```bash
pip install kedro-viz
```

Verify the installation:

```bash
kedro info

 _            _
| | _____  __| |_ __ ___
| |/ / _ \/ _` | '__/ _ \
|   <  __/ (_| | | | (_) |
|_|\_\___|\__,_|_|  \___/
v0.16.2

kedro allows teams to create analytics
projects. It is developed as part of
the Kedro initiative at QuantumBlack.

Installed plugins:
kedro_viz: 3.4.0 (hooks:global,line_magic)

```

# Create your Data Science project directory

We are going to create our datascience project directory using Kedro. It will initialize all the needed directories using a [cookiecutter](https://cookiecutter.readthedocs.io/) template.

First go to the directory where you're used to create your projects repos ( mine is /home/tdenimal/Dev )

```bash
cd /home/tdenimal/Dev
```

Then create project and choose the project name,  leave the other choices at their default value.
I choosed pneumothorax, to create the project directory for the [following work](https://tdenimal.github.io/projects/xray-classif_EDA/).

```bash

kedro new
Project Name:
=============
Please enter a human readable name for your new project.
Spaces and punctuation are allowed.
 [New Kedro Project]: pneumothorax

Repository Name:
================
Please enter a directory name for your new project repository.
Alphanumeric characters, hyphens and underscores are allowed.
Lowercase is recommended.
 [pneumothorax]: 

Python Package Name:
====================
Please enter a valid Python package name for your project package.
Alphanumeric characters and underscores are allowed.
Lowercase is recommended. Package name must start with a letter or underscore.
 [pneumothorax]: 

Generate Example Pipeline:
==========================
Do you want to generate an example pipeline in your project?
Good for first-time users. (default=N)
 [y/N]: 

Change directory to the project generated in /home/tdenimal/Dev/pneumothorax

A best-practice setup includes initialising git and creating a virtual environment before running `kedro install` to install project-specific dependencies. Refer to the Kedro documentation: https://kedro.readthedocs.io/


```

# GIT init 

As a best practice, you should add your newly created project directory to git right after creation:

```bash

cd pneumothorax

git init

git add --all

git commit
```

If you are a github user, you can create a repository with the same name on github and then push your local git.


First, you will need to add an ssh public key to your github profile.

You can generate a ssh key pair using

```bash
ssh-keygen
```

Then retrieve you public key from default location:

```bash
cat ~/.ssh/id_rsa.pub
```

Copy/paste the content of the .pub file to [your github profile](https://github.com/settings/keys)


Using this method, you will no longer need to submit your password each time you push your code to github.

Then push your code to github:

Just replace **username** with your github username.

```bash

git remote add origin git@github.com:username/${PWD##*/}

git push -u origin master
```





# Defining Data pipeline

## Install Dataset Dependencies

Our pipeline is using DICOMDataSet, to be able to read DICOM files and extract ImageDataSet and CSVDataSet from it. DICOMDataSet is a custom Kedro DataSet, you can see how it was created in the [following article](https://tdenimal.github.io/projects/kedro_custom_dataset/).


Kedro libraries dependencies are modularized, you can choose to install dependencies for a specic dataset.

For example, to install dependencies to be able to use the CSVDataSet:

```bash
pip install "kedro[pandas.CSVDataSet]"
```

Same thing for ImageDataSet which is using [pillow library](https://github.com/python-pillow/Pillow):

```bash
pip install "kedro[pillow.ImageDataSet]"
```

## Kedro Data Catalog

Before being able to used Dataset in Kedro you have to define them in the Kedro Data Catalog which regroups all datasets in our project.

You can configure it using *conf/base/catalog.yml* file.

By default, Kedro Data Catalog follows a Data Engineering Convention to classify datasets and managing data. From *raw* (our initials datasets)  to *model input* (ready to be used in our models).

It s a good practice to manage your datasets and group them  logically.

![](/assets/images/2020-08-10-kedro/data_engineering_convention.png)

You are free to follow this scheme or define yours. I decided to follow the default one as it suits my needs well.

Here's how Kedro team defines each layer : 


<table border="1" class="docutils">
<colgroup>
<col width="14%">
<col width="86%">
</colgroup>
<thead valign="bottom">
<tr><th class="head">Folder in data</th>
<th>Description</th>
</tr>
</thead>
<tbody valign="top">
<tr><td>Raw</td>
<td>Initial start of the pipeline, containing the sourced data model(s) that should never be changed,
it forms your single source of truth to work from. These data models are typically un-typed in
most cases e.g. csv, but this will vary from case to case.</td>
</tr>
<tr><td>Intermediate</td>
<td>Optional data model(s), which are introduced to type your <code><span>raw</span></code> data model(s), e.g.
converting string based values into their current typed representation.</td>
</tr>
<tr><td>Primary</td>
<td>Domain specific data model(s) containing cleansed, transformed and wrangled data from either
<code><span>raw</span></code> or <code ><span>intermediate</span></code>, which forms your layer that you input into your feature
engineering.</td>
</tr>
<tr><td>Feature</td>
<td>Analytics specific data model(s) containing a set of features defined against the <code><span>primary</span></code>
data, which are grouped by feature area of analysis and stored against a common dimension.</td>
</tr>
<tr><td>Model input</td>
<td>Analytics specific data model(s) containing all <code><span>feature</span></code> data against a common dimension
and in the case of live projects against an analytics run date to ensure that you track the
historical changes of the features over time.</td>
</tr>
<tr><td>Models</td>
<td>Stored, serialised pre-trained machine learning models.</td>
</tr>
<tr><td>Model output</td>
<td>Analytics specific data model(s) containing the results generated by the model based on the
<code><span>model</span> <span>input</span></code> data.</td>
</tr>
<tr><td>Reporting</td>
<td>Reporting data model(s) that are used to combine a set of <code><span>primary</span></code>, <code><span>feature</span></code>,
<code><span>model</span> <span>input</span></code> and <code><span>model</span> <span>output</span></code> data used to drive the dashboard and the views
constructed. It encapsulates and removes the need to define any blending or joining of data,
improve performance and replacement of presentation layer without having to redefine the data
models.</td>
</tr>
</tbody>
</table>


# Data engineering pipeline

## Step 1 - From .dcm files (raw) to .csv/.png files (intermediate/primary)

![](/assets/images/2020-08-10-kedro/data_pipeline1.png)

We are going to define the first part of our data engineering pipeline, working with dcm files 4, our *raw* dataset, to extract csv and png files.

Csv file is just an extract and concatenation of all metadata contained in dcm files. We will store it as *intermediate* dataset, as it is just a typing of our raw data.

The png files are extracted from dcm files but also resized for practical issues. We will store it as *model input* as it can be used directly without further modifications in our model.

### Defining raw dataset

In the *conf/base/catalog.yml* file, we will parameter our raw dataset *dicom_train*, the train directory containing dcm files.

```bash
dicom_train:
    type: PartitionedDataSet
    dataset: pneumothorax.io.datasets.dicom_dataset.DICOMDataSet
    path: data/01_raw/dicom-images-train
    filename_suffix: ".dcm"
```


*dicom_train* is typed as *PartitionedDataSet* as we are going to read the raw directory as a whole.

### Defining target datasets

In the *conf/base/catalog.yml* file, we will parameter our target datasets :
- *csv_train* :  a csv file containing concatenated dcm metadata 
- *img_train* : a directory to store .png files, extracted also from dcm files.

```bash
img_train:
    type: PartitionedDataSet
    dataset: pillow.ImageDataSet
    path: data/02_intermediate/img_train
    filename_suffix: ".png"

csv_train:
    type: pandas.CSVDataSet
    filepath: data/02_intermediate/train.csv

```

### Defining the pipeline node

```python
import pandas as pd
import numpy as np

from typing import List,Dict,Tuple
from PIL import Image


def array_to_img(array: np.ndarray, 
                size: Tuple[int, int] = (256,256)) -> Image:
    """Convert np.ndarray (pixel array) to PIL.Image and resizes eventually

    Args:
        array (np.ndarray): the input pixel array
        size (Tuple[int, int], optional): (width,height) of returned PIL.Image. Defaults to (256,256).

    Returns:
        Image: PIL.Image as output
    """
    return Image.fromarray(array).resize(size=(size[0], size[1]))



def preprocess_dicom(dicom: Dict) -> List:
    """Extract data from dicom.

    Args:
        dicom (Dict): dcm files ( PartitionedDataSet )

    Returns:
        List: [pandas.DataFrame (csv file), Dict of PIL.IMage ( PartitionedDataSet )
    """

    #Init data with first dict item
    imgs = {}

    partition_id, partition_load_func = dicom.popitem()
    partition_data = partition_load_func()

    csv = partition_data[0]
    imgs[partition_id] = array_to_img(partition_data[1])
    
    #Loop on dict to extract img and csv data
    for partition_id, partition_load_func in dicom.items():
        partition_data = partition_load_func()

        imgs[partition_id] = array_to_img(partition_data[1])

        csv = pd.concat(
            [csv, partition_data[0]], ignore_index=True, sort=True
        )

    return [csv,imgs]

```




